{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "beer advocate selenium&bs4 scraper.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKS9t_8Q6oDX",
        "colab_type": "code",
        "outputId": "e6ff1cba-131f-4d52-fefd-16bdd53148af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Use this code if using Colab to run Selenium\n",
        "\n",
        "# Make sure to go to Runtime -> Change runtime type and select Python 3 as runtime type and GPU as hardware accelerator\n",
        "\n",
        "# !kill -9 -1 # Use this line to delete this VM and start a new one. \n",
        "# The above line deletes all files and folders from the current VM and allocates a new one.\n",
        "!apt-get update\n",
        "!pip install beautifulsoup4\n",
        "!pip install selenium\n",
        "# !apt-get -q update # to update ubuntu to correctly run apt install\n",
        "!apt install -yq chromium-chromedriver\n",
        "!cp /usr/lib/chromium-browser/chromedriver /usr/bin\n",
        "\n",
        "from selenium import webdriver\n",
        "from selenium.webdriver.support.ui import WebDriverWait\n",
        "from selenium.webdriver.support import expected_conditions as EC\n",
        "from selenium.webdriver.common.by import By\n",
        "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
        "from bs4 import BeautifulSoup, Tag, NavigableString\n",
        "from selenium.webdriver.chrome.options import Options\n",
        "import csv\n",
        "import random\n",
        "import time\n",
        "import pandas as pd\n",
        "\n",
        "import sys\n",
        "sys.path.insert(0,'/usr/lib/chromium-browser/chromedriver')\n",
        "from selenium import webdriver"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ign:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:2 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:6 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Hit:7 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Hit:8 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Hit:10 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [5,944 B]\n",
            "Get:15 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [782 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,301 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [12.6 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [695 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [990 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [23.2 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [9,022 B]\n",
            "Fetched 4,075 kB in 5s (802 kB/s)\n",
            "Reading package lists... Done\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.6/dist-packages (4.6.3)\n",
            "Collecting selenium\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/80/d6/4294f0b4bce4de0abf13e17190289f9d0613b0a44e5dd6a7f5ca98459853/selenium-3.141.0-py2.py3-none-any.whl (904kB)\n",
            "\u001b[K     |████████████████████████████████| 911kB 5.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3 in /usr/local/lib/python3.6/dist-packages (from selenium) (1.24.3)\n",
            "Installing collected packages: selenium\n",
            "Successfully installed selenium-3.141.0\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "The following additional packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-codecs-ffmpeg-extra\n",
            "Suggested packages:\n",
            "  webaccounts-chromium-extension unity-chromium-extension adobe-flashplugin\n",
            "The following NEW packages will be installed:\n",
            "  chromium-browser chromium-browser-l10n chromium-chromedriver\n",
            "  chromium-codecs-ffmpeg-extra\n",
            "0 upgraded, 4 newly installed, 0 to remove and 30 not upgraded.\n",
            "Need to get 70.8 MB of archives.\n",
            "After this operation, 254 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-codecs-ffmpeg-extra amd64 77.0.3865.90-0ubuntu0.18.04.1 [1,079 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser amd64 77.0.3865.90-0ubuntu0.18.04.1 [62.3 MB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-browser-l10n all 77.0.3865.90-0ubuntu0.18.04.1 [3,011 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 chromium-chromedriver amd64 77.0.3865.90-0ubuntu0.18.04.1 [4,388 kB]\n",
            "Fetched 70.8 MB in 8s (8,423 kB/s)\n",
            "Selecting previously unselected package chromium-codecs-ffmpeg-extra.\n",
            "(Reading database ... 132681 files and directories currently installed.)\n",
            "Preparing to unpack .../chromium-codecs-ffmpeg-extra_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-codecs-ffmpeg-extra (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser.\n",
            "Preparing to unpack .../chromium-browser_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-browser (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-browser-l10n.\n",
            "Preparing to unpack .../chromium-browser-l10n_77.0.3865.90-0ubuntu0.18.04.1_all.deb ...\n",
            "Unpacking chromium-browser-l10n (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Selecting previously unselected package chromium-chromedriver.\n",
            "Preparing to unpack .../chromium-chromedriver_77.0.3865.90-0ubuntu0.18.04.1_amd64.deb ...\n",
            "Unpacking chromium-chromedriver (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\n",
            "Setting up chromium-codecs-ffmpeg-extra (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for hicolor-icon-theme (0.17-2) ...\n",
            "Setting up chromium-browser (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\n",
            "update-alternatives: using /usr/bin/chromium-browser to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\n",
            "Setting up chromium-chromedriver (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "Setting up chromium-browser-l10n (77.0.3865.90-0ubuntu0.18.04.1) ...\n",
            "cp: '/usr/lib/chromium-browser/chromedriver' and '/usr/bin/chromedriver' are the same file\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Ao57zl38YFL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SeleniumCraigslistScraper(object):\n",
        "    pass\n",
        "\n",
        "    def __init__(self, number_reviews_scrape):\n",
        "        chrome_options = Options()\n",
        "        chrome_options.add_argument('--headless')\n",
        "        chrome_options.add_argument('--no-sandbox')\n",
        "        chrome_options.add_argument('--disable-dev-shm-usage')\n",
        "        self.driver = webdriver.Chrome(options=chrome_options)\n",
        "        self.delay = 5\n",
        "        self.reviews = []\n",
        "        self.beers = []\n",
        "        self.number_reviews_scrape = number_reviews_scrape\n",
        "\n",
        "    def go_to_page(self, url, element_type=None, element_value=None):\n",
        "        try:\n",
        "            self.driver.get(url)\n",
        "            if element_type is not None or element_value is not None:\n",
        "              if element_type == 'id':\n",
        "                  self.wait(element_id=element_value)\n",
        "              elif element_type == 'name':\n",
        "                  self.wait(element_name=element_value)\n",
        "        except TimeoutException:\n",
        "            print('Loading took too long')\n",
        "        except NoSuchElementException:\n",
        "            print(f\"Couldn\\'t find {element_value} D:\")\n",
        "        except Exception as e:\n",
        "            print(str(e))\n",
        "\n",
        "\n",
        "    def wait(self, element_id=None, element_class_name=None, element_name=None):\n",
        "        delay = random.randint(1, 6)\n",
        "        wait = WebDriverWait(self.driver, delay)\n",
        "        if element_id is not None:\n",
        "            wait.until(EC.presence_of_element_located((By.ID, element_id)))\n",
        "        if element_class_name is not None:\n",
        "            wait.until(EC.presence_of_element_located((By.CLASS_NAME, element_class_name)))\n",
        "        if element_name is not None:\n",
        "            wait.until(EC.presence_of_element_located((By.NAME, element_name)))\n",
        "        else:\n",
        "            time.sleep(delay)\n",
        "\n",
        "    def get_list_of_beers(self):\n",
        "        beers = self.driver.find_element_by_id('ba-content').find_element_by_tag_name('table').find_elements_by_tag_name('tr')\n",
        "        self.beers = beers[1:len(beers)]  # removes the first <tr> (doesn't contain beer)\n",
        "\n",
        "    def iterate_through_beers(self):\n",
        "        init_count = len(self.beers)\n",
        "        for i in range(1, init_count):\n",
        "            if len(self.reviews) < self.number_reviews_scrape:\n",
        "                # get page source to prevent stale elements error\n",
        "                beers = self.driver.find_element_by_id('ba-content').find_element_by_tag_name('table').find_elements_by_tag_name('tr')\n",
        "                beer = beers[i]\n",
        "                beer.find_elements_by_tag_name('td')[1].find_element_by_tag_name('a').click()\n",
        "                self.scrape_page_reviews()\n",
        "                self.driver.execute_script(\"window.history.go(-1)\")\n",
        "                self.wait()\n",
        "\n",
        "    def scrape_page_reviews(self):\n",
        "        html = self.driver.page_source\n",
        "        soup = BeautifulSoup(html, \"lxml\")\n",
        "        product_name = soup.find('div', {'class': 'titleBar'})#.h1.text\n",
        "        for span in product_name.findAll('span'):\n",
        "            span.decompose()\n",
        "        product_name = soup.find('div', {'class': 'titleBar'}).h1.text\n",
        "        for user_review in soup.find_all('div', {'id': 'rating_fullview_content_2'}):\n",
        "            user_score = user_review.find('span', {'class': 'BAscore_norm'}).text\n",
        "            out_of_score = user_review.find('span', {'class': 'rAvg_norm'}).text.split('/')[1]\n",
        "\n",
        "            for thing in user_review.find_all(['br', 'i', 'span', 'div']):\n",
        "                thing.decompose()\n",
        "\n",
        "            review_text = user_review.text\n",
        "            if 'rDev' in review_text[0:10]:\n",
        "                review_text = review_text[6:]\n",
        "                \n",
        "            review_text = review_text.strip()\n",
        "            \n",
        "            if '=' in review_text[0:1]:\n",
        "                review_text = review_text[1:]\n",
        "                \n",
        "            review_text = self.deEmojify(review_text)\n",
        "            \n",
        "            self.reviews.append({'product name': product_name, 'product review': review_text, 'user rating': user_score})\n",
        "            if len(self.reviews) % 100 == 0:\n",
        "              print(f'have collected {len(self.reviews)} reviews')\n",
        "\n",
        "    def deEmojify(self, inputString):\n",
        "        return inputString.encode('ascii', 'ignore').decode('ascii')\n",
        "\n",
        "    def create_pandas_csv(self, filename='beerviews.csv'):\n",
        "        df = pd.DataFrame(self.reviews)\n",
        "        df.to_csv(filename, encoding='utf-8', index=False)\n",
        "\n",
        "    def quit_driver(self):\n",
        "        self.driver.quit()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Qr0iJYi6piD",
        "colab_type": "code",
        "outputId": "599f5535-9672-47be-c294-d2d86c73f527",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 867
        }
      },
      "source": [
        "url = 'https://www.beeradvocate.com/beer/top-rated/'\n",
        "number_reviews_scrape = 5000\n",
        "try:  \n",
        "  scraper = SeleniumCraigslistScraper(number_reviews_scrape=number_reviews_scrape)\n",
        "  scraper.go_to_page(url, 'id', 'ba-content')\n",
        "  scraper.get_list_of_beers()\n",
        "  scraper.iterate_through_beers()\n",
        "  scraper.quit_driver()\n",
        "  scraper.create_pandas_csv('beerreviews.csv')\n",
        "except Exception as e:\n",
        "  print(str(e))\n",
        "  scraper.quit_driver()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "have collected 100 reviews\n",
            "have collected 200 reviews\n",
            "have collected 300 reviews\n",
            "have collected 400 reviews\n",
            "have collected 500 reviews\n",
            "have collected 600 reviews\n",
            "have collected 700 reviews\n",
            "have collected 800 reviews\n",
            "have collected 900 reviews\n",
            "have collected 1000 reviews\n",
            "have collected 1100 reviews\n",
            "have collected 1200 reviews\n",
            "have collected 1300 reviews\n",
            "have collected 1400 reviews\n",
            "have collected 1500 reviews\n",
            "have collected 1600 reviews\n",
            "have collected 1700 reviews\n",
            "have collected 1800 reviews\n",
            "have collected 1900 reviews\n",
            "have collected 2000 reviews\n",
            "have collected 2100 reviews\n",
            "have collected 2200 reviews\n",
            "have collected 2300 reviews\n",
            "have collected 2400 reviews\n",
            "have collected 2500 reviews\n",
            "have collected 2600 reviews\n",
            "have collected 2700 reviews\n",
            "have collected 2800 reviews\n",
            "have collected 2900 reviews\n",
            "have collected 3000 reviews\n",
            "have collected 3100 reviews\n",
            "have collected 3200 reviews\n",
            "have collected 3300 reviews\n",
            "have collected 3400 reviews\n",
            "have collected 3500 reviews\n",
            "have collected 3600 reviews\n",
            "have collected 3700 reviews\n",
            "have collected 3800 reviews\n",
            "have collected 3900 reviews\n",
            "have collected 4000 reviews\n",
            "have collected 4100 reviews\n",
            "have collected 4200 reviews\n",
            "have collected 4300 reviews\n",
            "have collected 4400 reviews\n",
            "have collected 4500 reviews\n",
            "have collected 4600 reviews\n",
            "have collected 4700 reviews\n",
            "have collected 4800 reviews\n",
            "have collected 4900 reviews\n",
            "have collected 5000 reviews\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}